<!DOCTYPE html>
<html>
  <head>
    <base target="_blank">
    <title>Docker Orchestration Workshop</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }

      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
        margin-top: 0.5em;
      }
      a {
        text-decoration: none;
        color: blue;
      }
      .remark-slide-content { padding: 1em 2.5em 1em 2.5em; }

      .remark-slide-content { font-size: 25px; }
      .remark-slide-content h1 { font-size: 50px; }
      .remark-slide-content h2 { font-size: 50px; }
      .remark-slide-content h3 { font-size: 25px; }
      .remark-code { font-size: 25px; }
      .small .remark-code { font-size: 16px; }

      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .red { color: #fa0000; }
      .gray { color: #ccc; }
      .small { font-size: 70%; }
      .big { font-size: 140%; }
      .underline { text-decoration: underline; }
      .strike { text-decoration: line-through; }
      .pic {
        vertical-align: middle;
        text-align: center;
        padding: 0 0 0 0 !important;
      }
      img {
        max-width: 100%;
        max-height: 550px;
      }
      .title {
        vertical-align: middle;
        text-align: center;
      }
      .title h1 { font-size: 100px; }
      .title p { font-size: 100px; }
      .quote {
        background: #eee;
        border-left: 10px solid #ccc;
        margin: 1.5em 10px;
        padding: 0.5em 10px;
        quotes: "\201C""\201D""\2018""\2019";
        font-style: italic;
      }
      .quote:before {
        color: #ccc;
        content: open-quote;
        font-size: 4em;
        line-height: 0.1em;
        margin-right: 0.25em;
        vertical-align: -0.4em;
      }
      .quote p {
        display: inline;
      }
      .warning {
        background-image: url("warning.png");
        background-size: 1.5em;
        background-repeat: no-repeat;
        padding-left: 2em;
      }
      .exercise {
        background-color: #eee;
        background-image: url("keyboard.png");
        background-size: 1.4em;
        background-repeat: no-repeat;
        background-position: 0.2em 0.2em;
        border: 2px dotted black;
      }
      .exercise::before {
        content: "Exercise";
        margin-left: 1.8em;
      }
      li p { line-height: 1.25em; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: title

Docker <br/> Orchestration <br/> Workshop

???
???

## Intros

- Hello! We are
  AJ ([@s0ulshake](https://twitter.com/s0ulshake))
  &
  J√©r√¥me ([@jpetazzo](https://twitter.com/jpetazzo))

???

- This is our collective Docker knowledge:

  ![Bell Curve](bell-curve.jpg)

<!--
Reminder, when updating the agenda: when people are told to show
up at 9am, they usually trickle in until 9:30am (except for paid
training sessions). If you're not sure that people will be there
on time, it's a good idea to have a breakfast with the attendees
at e.g. 9am, and start at 9:30.
-->

???

## Agenda

<!--
- Agenda:
-->

.small[
- 09:00-09:15 hello
- 09:15-10:45 part 1
- 10:45-11:00 coffee break
- 11:00-12:30 part 2
- 12:30-13:30 lunch break
- 13:30-15:00 part 3
- 15:00-15:15 coffee break
- 15:15-16:45 part 4
- 16:45-17:00 Q&A
]

<!--
- The tutorial will run from 1pm to 5pm
- This will be fast-paced, but DON'T PANIC!
- We will do short breaks for coffee + QA every hour
-->

- Feel free to interrupt for questions at any time

- Live feedback, questions, help on
  [Slack](http://container.training/chat)
  ([get an invite](http://lisainvite.herokuapp.com/))

- All the content is publicly available (slides, code samples, scripts)

<!--
Remember to change:
- the link above
- the "tweet my speed" hashtag in DockerCoins HTML
-->

---

<!--
grep '^# ' index.html | grep -v '<br' | tr '#' '-'
-->

## Chapter 1: getting started

- Pre-requirements
- VM environment
- Our sample application
- Running the application
- Introducing SwarmKit

---

## Chapter 2: scaling out our app on Swarm

- Creating our first Swarm
- Running our first Swarm service
- Deploying a local registry
- Overlay networks
- Breaking into an overlay network
- Securing overlay networks

???

## Chapter 3: operating the Swarm

- Rolling updates
- Secrets management and encryption at rest
- Metrics collection

---

## Chapter 3: deeper in Swarm

- Dealing with stateful services
- Scripting image building and pushing
- Integration with Compose
- Controlling Docker from a container
- Node management
- What's next?

???

# Pre-requirements

- Computer with network connection and SSH client

  - on Linux, OS X, FreeBSD... you are probably all set

  - on Windows, get [putty](http://www.putty.org/),
  Microsoft [Win32 OpenSSH](https://github.com/PowerShell/Win32-OpenSSH/wiki/Install-Win32-OpenSSH),
  [Git BASH](https://git-for-windows.github.io/), or
  [MobaXterm](http://mobaxterm.mobatek.net/)

- Some Docker knowledge

  (but that's OK if you're not a Docker expert!)

---

## Nice-to-haves

- [Mosh](https://mosh.org/) if your internet connection tends to lose packets
  <br/>(available with `(apt|yum|brew) install mosh`; then use it instead of SSH)

- [GitHub](https://github.com/join) account
  <br/>(if you want to fork the repo; also used to join Gitter)

<!--

- [Gitter](https://gitter.im/) account
  <br/>(to join the conversation during the workshop)

-->

- [Docker Hub](https://hub.docker.com) account
  <br/>(it's one way to distribute images on your Swarm cluster)

---

## Hands-on sections

- The whole workshop is hands-on

- We will see Docker 1.13 in action

- You are invited to reproduce all the demos

- All hands-on sections are clearly identified, like the gray rectangle below

.exercise[

- This is the stuff you're supposed to do!

]

---

# VM environment

- To follow along, you need a cluster of five Docker Engines

- If you are doing this with an instructor, see next slide

- If you are doing (or re-doing) this on your own, you can:

  - create your own cluster (local or cloud VMs) with Docker Machine
    ([instructions](https://github.com/ebriand/orchestration-workshop/tree/master/prepare-machine))

  - use [Play-With-Docker](http://play-with-docker.com) ([instructions](https://github.com/ebriand/orchestration-workshop#using-play-with-docker))

  - create a bunch of clusters for you and your friends
    ([instructions](https://github.com/ebriand/orchestration-workshop/tree/master/prepare-vms))

---

class: pic

![You get five VMs](you-get-five-vms.jpg)

---

## You get five VMs

- Each person gets 5 private VMs (not shared with anybody else)
- They'll remain up until the day after the tutorial
- You should have a little card with login+password+IP addresses
- You can automatically SSH from one VM to another

.exercise[

<!--
```bash
for N in $(seq 1 5); do
  ssh -o StrictHostKeyChecking=no node$N true
done
for N in $(seq 1 5); do
  (.
  docker-machine rm -f node$N
  ssh node$N "docker ps -aq | xargs -r docker rm -f"
  ssh node$N sudo rm -f /etc/systemd/system/docker.service
  ssh node$N sudo systemctl daemon-reload
  echo Restarting node$N.
  ssh node$N sudo systemctl restart docker
  echo Restarted node$N.
  ) &
done
wait
```
-->

- Log into the first VM (`node1`) with SSH or MOSH
- Check that you can SSH (without password) to `node2`:
  ```bash
  ssh node2
  ```
- Type `exit` or `^D` to come back to node1

<!--
```meta
^D
```
-->

]

---

## If doing or re-doing the workshop on your own ...

- If you use Play-With-Docker:

  - you can't SSH to the machines

  - you access the terminal directly in your browser

  - exposing services requires something like
    [ngrok](https://ngrok.com/)
    or [supergrok](https://github.com/ebriand/orchestration-workshop#using-play-with-docker)

- If you use VMs deployed with Docker Machine:

  - you won't have pre-authorized SSH keys to bounce across machines

  - you won't have host aliases

---

## We will (mostly) interact with node1 only

- Unless instructed, **all commands must be run from the first VM, `node1`**

- We will only checkout/copy the code on `node1`

- When we will use the other nodes, we will do it mostly through the Docker API

- We will use SSH only for the initial setup and a few "out of band" operations
  <br/>(checking internal logs, debugging...)

---

## Terminals

Once in a while, the instructions will say:
<br/>"Open a new terminal."

There are multiple ways to do this:

- create a new window or tab on your machine, and SSH into the VM;

- use screen or tmux on the VM and open a new window from there.

You are welcome to use the method that you feel the most comfortable with.

???

## Tmux cheatsheet

- Ctrl-b c ‚Üí creates a new window
- Ctrl-b n ‚Üí go to next window
- Ctrl-b p ‚Üí go to previous window
- Ctrl-b " ‚Üí split window top/bottom
- Ctrl-b % ‚Üí split window left/right
- Ctrl-b Alt-1 ‚Üí rearrange windows in columns
- Ctrl-b Alt-2 ‚Üí rearrange windows in rows
- Ctrl-b arrows ‚Üí navigate to other windows
- Ctrl-b d ‚Üí detach session
- tmux attach ‚Üí reattach to session

---

## Brand new versions!

- Engine 1.13-rc
- Compose 1.9.0
- Machine 0.9-rc

.exercise[

- Check all installed versions:
  ```bash
  docker version
  docker-compose -v
  docker-machine -v
  ```

]

---

# Our sample application

- Visit the GitHub repository with all the materials of this workshop:
  <br/>https://github.com/ebriand/orchestration-workshop

- The application is in the [dockercoins](
  https://github.com/ebriand/orchestration-workshop/tree/master/dockercoins)
  subdirectory

- Let's look at the general layout of the source code:

  there is a Compose file [docker-compose.yml](
  https://github.com/ebriand/orchestration-workshop/blob/master/dockercoins/docker-compose.yml) ...

  ... and 4 other services, each in its own directory:

  - `rng` = web service generating random bytes
  - `hasher` = web service computing hash of POSTed data
  - `worker` = background process using `rng` and `hasher`
  - `webui` = web interface to watch progress

???
---

## Compose file format version

*Particularly relevant if you have used Compose before...*

- Compose 1.6 introduced support for a new Compose file format (aka "v2")

- Services are no longer at the top level, but under a `services` section

- There has to be a `version` key at the top level, with value `"2"` (as a string, not an integer)

- Containers are placed on a dedicated network, making links unnecessary

- There are other minor differences, but upgrade is easy and straightforward

---

## Links, naming, and service discovery

- Containers can have network aliases (resolvable through DNS)

- Compose file version 2 makes each container reachable through its service name

- Compose file version 1 requires "links" sections

- Our code can connect to services using their short name

  (instead of e.g. IP address or FQDN)

---

## Example in `worker/worker.py`

![Service discovery](service-discovery.png)

---

## What's this application?

---

class: pic

![DockerCoins logo](dockercoins.png)

(DockerCoins 2016 logo courtesy of [@XtlCnslt](https://twitter.com/xtlcnslt) and [@ndeloof](https://twitter.com/ndeloof). Thanks!)

---

## What's this application?

- It is a DockerCoin miner! üí∞üê≥üì¶üö¢

- No, you can't buy coffee with DockerCoins

- How DockerCoins works:

  - `worker` asks to `rng` to give it random bytes
  - `worker` feeds those random bytes into `hasher`
  - each hash starting with `0` is a DockerCoin
  - DockerCoins are stored in `redis`
  - `redis` is also updated every second to track speed
  - you can see the progress with the `webui`

---

## Getting the application source code

- We will clone the GitHub repository

- The repository also contains scripts and tools that we will use through the workshop

.exercise[

<!--
```bash
[ -d orchestration-workshop ] && mv orchestration-workshop orchestration-workshop.$$
```
-->

- Clone the repository on `node1`:
  ```bash
  git clone git://github.com/ebriand/orchestration-workshop
  ```

]

(You can also fork the repository on GitHub and clone your fork if you prefer that.)

---

# Running the application

Without further ado, let's start our application.

.exercise[

- Go to the `dockercoins` directory, in the cloned repo:
  ```bash
  cd ~/orchestration-workshop/dockercoins
  ```

- Use Compose to build and run all containers:
  ```bash
  docker-compose up
  ```

]

Compose tells Docker to build all container images (pulling
the corresponding base images), then starts all containers,
and displays aggregated logs.

---

## Lots of logs

- The application continuously generates logs

- We can see the `worker` service making requests to `rng` and `hasher`

- Let's put that in the background

.exercise[

- Stop the application by hitting `^C`

<!--
```meta
^C
```
-->

]

- `^C` stops all containers by sending them the `TERM` signal

- Some containers exit immediately, others take longer
  <br/>(because they don't handle `SIGTERM` and end up being killed after a 10s timeout)

---

## Restarting in the background

- Many flags and commands of Compose are modeled after those of `docker`

.exercise[

- Start the app in the background with the `-d` option:
  ```bash
  docker-compose up -d
  ```

- Check that our app is running with the `ps` command:
  ```bash
  docker-compose ps
  ```

]

`docker-compose ps` also shows the ports exposed by the application.

???
---

## Viewing logs

- The `docker-compose logs` command works like `docker logs`

.exercise[

- View all logs since container creation and exit when done:
  ```bash
  docker-compose logs
  ```

- Stream container logs, starting at the last 10 lines for each container:
  ```bash
  docker-compose logs --tail 10 --follow
  ```

<!--
```meta
^C
```
-->

]

Tip: use `^S` and `^Q` to pause/resume log output.

???
???

## Upgrading from Compose 1.6

.warning[The `logs` command has changed between Compose 1.6 and 1.7!]

- Up to 1.6

  - `docker-compose logs` is the equivalent of `logs --follow`

  - `docker-compose logs` must be restarted if containers are added

- Since 1.7

  - `--follow` must be specified explicitly

  - new containers are automatically picked up by `docker-compose logs`

---

## Connecting to the web UI

- The `webui` container exposes a web dashboard; let's view it

.exercise[

- Open http://[yourVMaddr]:8000/ (from a browser)

]

- The app actually has a constant, steady speed (3.33 coins/second)

- The speed seems not-so-steady because:

  - the worker doesn't update the counter after every loop, but up to once per second

  - the speed is computed by the browser, checking the counter about once per second

  - between two consecutive updates, the counter will increase either by 4, or by 0

???

## Scaling up the application

- Our goal is to make that performance graph go up (without changing a line of code!)

??
- Before trying to scale the application, we'll figure out if we need more resources

  (CPU, RAM...)

- For that, we will use good old UNIX tools on our Docker node

???

## Looking at resource usage

- Let's look at CPU, memory, and I/O usage

.exercise[

- run `top` to see CPU and memory usage (you should see idle cycles)

- run `vmstat 3` to see I/O usage (si/so/bi/bo)
  <br/>(the 4 numbers should be almost zero, except `bo` for logging)

]

We have available resources.

- Why?
- How can we use them?

---

## Scaling workers on a single node

- Docker Compose supports scaling
- Let's scale `worker` and see what happens!

.exercise[

- Start one more `worker` container:
  ```bash
  docker-compose scale worker=2
  ```

- Look at the performance graph (it should show a x2 improvement)

- Look at the aggregated logs of our containers (`worker_2` should show up)

- Look at the impact on CPU load with e.g. top (it should be negligible)

]

---

## Adding more workers

- Great, let's add more workers and call it a day, then!

.exercise[

- Start eight more `worker` containers:
  ```bash
  docker-compose scale worker=10
  ```

- Look at the performance graph: does it show a x10 improvement?

- Look at the aggregated logs of our containers

- Look at the impact on CPU load and memory usage

<!--
```bash
sleep 5
killall docker-compose
```
-->

]

???

# Identifying bottlenecks

- You should have seen a 3x speed bump (not 10x)

- Adding workers didn't result in linear improvement

- *Something else* is slowing us down



- ... But what?


- The code doesn't have instrumentation

- Let's use state-of-the-art HTTP performance analysis!
  <br/>(i.e. good old tools like `ab`, `httping`...)

???

## Accessing internal services

- `rng` and `hasher` are exposed on ports 8001 and 8002

- This is declared in the Compose file:

  ```yaml
    ...
    rng:
      build: rng
      ports:
      - "8001:80"

    hasher:
      build: hasher
      ports:
      - "8002:80"
    ...
  ```

???

## Measuring latency under load

We will use `httping`.

.exercise[

- Check the latency of `rng`:
  ```bash
  httping -c 10 localhost:8001
  ```

- Check the latency of `hasher`:
  ```bash
  httping -c 10 localhost:8002
  ```

]

`rng` has a much higher latency than `hasher`.

???

## Let's draw hasty conclusions

- The bottleneck seems to be `rng`

- *What if* we don't have enough entropy and can't generate enough random numbers?

- We need to scale out the `rng` service on multiple machines!

Note: this is a fiction! We have enough entropy. But we need a pretext to scale out.

(In fact, the code of `rng` uses `/dev/urandom`, which never runs out of entropy...
<br/>
...and is [just as good as `/dev/random`](http://www.slideshare.net/PacSecJP/filippo-plain-simple-reality-of-entropy).)

---

## Clean up

- Before moving on, let's remove those containers

.exercise[

- Tell Compose to remove everything:
  ```bash
  docker-compose down
  ```

]

---

class: title

#  Scaling out

---

# SwarmKit

- [SwarmKit](https://github.com/docker/swarmkit) is an open source
  toolkit to build multi-node systems

- It is a reusable library, like libcontainer, libnetwork, vpnkit ...

- It is a plumbing part of the Docker ecosystem

- SwarmKit comes with two examples:

  - `swarmctl` (a CLI tool to "speak" the SwarmKit API)

  - `swarmd` (an agent that can federate existing Docker Engines into a Swarm)

- SwarmKit/swarmd/swarmctl ‚Üí libcontainer/containerd/container-ctr

---

## SwarmKit features

- Highly-available, distributed store based on [Raft](
  https://en.wikipedia.org/wiki/Raft_(computer_science))
  <br/>(more on next slide)

- *Services* managed with a *declarative API*
  <br/>(implementing *desired state* and *reconciliation loop*)

- Automatic TLS keying and signing

- Dynamic promotion/demotion of nodes, allowing to change
  how many nodes are actively part of the Raft consensus

- Integration with overlay networks and load balancing

- And much more!

---

## Where is the key/value store?

- Many orchestration systems use a key/value store backed by a consensus algorithm
  <br/>
  (k8s‚Üíetcd‚ÜíRaft, mesos‚Üízookeeper‚ÜíZAB, etc.)

- SwarmKit implements the Raft algorithm directly
  <br/>
  (Nomad is similar; thanks [@cbednarski](https://twitter.com/@cbednarski),
  [@diptanu](https://twitter.com/diptanu) and others for point it out!)

- Analogy courtesy of [@aluzzardi](https://twitter.com/aluzzardi):

  *It's like B-Trees and RDBMS. They are different layers, often
  associated. But you don't need to bring up a full SQL server when
  all you need is to index some data.*

- As a result, the orchestrator has direct access to the data
  <br/>
  (the main copy of the data is stored in the orchestrator's memory)

- Simpler, easier to deploy and operate; also faster

---

## SwarmKit concepts (1/2)

- A *cluster* will be at least one *node* (preferably more)

- A *node* can be a *manager* or a *worker*

  (Note: in SwarmKit, *managers* are also *workers*)

- A *manager* actively takes part in the Raft consensus

- You can talk to a *manager* using the SwarmKit API

- One *manager* is elected as the *leader*; other managers merely forward requests to it

---

## Illustration

![Illustration](swarm-mode.svg)

---

## SwarmKit concepts (2/2)

- The *managers* expose the SwarmKit API

- Using the API, you can indicate that you want to run a *service*

- A *service* is specified by its *desired state*: which image, how many instances...

- The *leader* uses different subsystems to break down services into *tasks*:
  <br/>orchestrator, scheduler, allocator, dispatcher

- A *task* corresponds to a specific container, assigned to a specific *node*

- *Nodes* know which *tasks* should be running, and will start or stop containers accordingly (through the Docker Engine API)

You can refer to the [NOMENCLATURE](https://github.com/docker/swarmkit/blob/master/design/nomenclature.md) in the SwarmKit repo for more details.

---

## Swarm Mode

- Since version 1.12, Docker Engine embeds SwarmKit

- The Docker CLI features three new commands:

  - `docker swarm` (enable Swarm mode; join a Swarm; adjust cluster parameters)

  - `docker node` (view nodes; promote/demote managers; manage nodes)

  - `docker service` (create and manage services)

- The Docker API exposes the same concepts

- The SwarmKit API is also exposed (on a separate socket)

---

## You need to enable Swarm mode to use the new stuff

- By default, all this new code is inactive

- Swarm Mode can be enabled, "unlocking" SwarmKit functions
  <br/>(services, out-of-the-box overlay networks, etc.)

.exercise[

- Try a Swarm-specific command:
  ```bash
  docker node ls
  ```

]

--

You will get an error message:
```
Error response from daemon: This node is not a swarm manager. [...]
```

---

# Creating our first Swarm

- The cluster is initialized with `docker swarm init`

- This should be executed on a first, seed node

- .warning[DO NOT execute `docker swarm init` on multiple nodes!]

  You would have multiple disjoint clusters.

.exercise[

- Create our cluster from node1:
  ```bash
  docker swarm init
  ```

]

If Docker tells you that it `could not choose an IP address to advertise`, see next slide!

---

## IP address to advertise

- When running in Swarm mode, each node *advertises* its address to the others
  <br/>
  (i.e. it tells them *"you can contact me on 10.1.2.3:2377"*)

- If the node has only one IP address (other than 127.0.0.1), it is used automatically

- If the node has multiple IP addresses, you **must** specify which one to use
  <br/>
  (Docker refuses to pick one randomly)

- You can specify an IP address or an interface name
  <br/>(in the latter case, Docker will read the IP address of the interface and use it)

- You can also specify a port number
  <br/>(otherwise, the default port 2377 will be used)

---

## Which IP address should be advertised?

- If your nodes have only one IP address, it's safe to let autodetection do the job

  .small[(Except if your instances have different private and public addresses, e.g.
  on EC2, and you are building a Swarm involving nodes inside and outside the
  private network: then you should advertise the public address.)]

- If your nodes have multiple IP addresses, pick an address which is reachable
  *by every other node* of the Swarm

- If you are using [play-with-docker](http://play-with-docker.com/), use the IP
  address shown next to the node name

  .small[(This is the address of your node on your private internal overlay network.
  The other address that you might see is the address of your node on the
  `docker_gwbridge` network, which is used for outbound traffic.)]

Examples:

```bash
docker swarm init --advertise-addr 10.0.9.2
docker swarm init --advertise-addr eth0:7777
```

---

## Token generation

- In the output of `docker swarm init`, we have a message
  confirming that our node is now the (single) manager:

  ```
  Swarm initialized: current node (8jud...) is now a manager.
  ```

- Docker generated two security tokens (like passphrases or passwords) for our cluster

- The CLI shows us the command to use on other nodes to add them to the cluster using the "worker"
  security token:

  ```
    To add a worker to this swarm, run the following command:
      docker swarm join \
      --token SWMTKN-1-59fl4ak4nqjmao1ofttrc4eprhrola2l87... \
      172.31.4.182:2377
  ```

---

## Checking that Swarm mode is enabled

.exercise[

- Run the traditional `docker info` command:
  ```bash
  docker info
  ```

]

The output should include:

```
Swarm: active
 NodeID: 8jud7o8dax3zxbags3f8yox4b
 Is Manager: true
 ClusterID: 2vcw2oa9rjps3a24m91xhvv0c
 ...
```

---

## Running our first Swarm mode command

- Let's retry the exact same command as earlier

.exercise[

- List the nodes (well, the only node) of our cluster:
  ```bash
  docker node ls
  ```

]

The output should look like the following:
```
ID             HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
8jud...ox4b *  node1     Ready   Active        Leader
```

---

## Adding nodes to the Swarm

- A cluster with one node is not a lot of fun

- Let's add `node2`!

- We need the token that was shown earlier

--

- You wrote it down, right?

--

- Don't panic, we can easily see it again üòè

---

## Adding nodes to the Swarm

.exercise[

- Show the token again:
  ```bash
  docker swarm join-token worker
  ```

- Log into `node2`:
  ```bash
  ssh node2
  ```

- Copy paste the `docker swarm join ...` command
  <br/>(that was displayed just before)

]

???
---

## Check that the node was added correctly

- Stay logged into `node2`!

.exercise[

- We can still use `docker info` to verify that the node is part of the Swarm:
  ```bash
  docker info | grep ^Swarm
  ```

]

- However, Swarm commands will not work; try, for instance:
  ```
  docker node ls
  ```

- This is because the node that we added is currently a *worker*

- Only *managers* can accept Swarm-specific commands

---

## View our two-node cluster

- Let's go back to `node1` and see what our cluster looks like

.exercise[

- Logout from `node2` (with `exit` or `Ctrl-D` or ...)

- View the cluster from `node1`, which is a manager:
  ```bash
  docker node ls
  ```

]

The output should be similar to the following:
```
ID             HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
8jud...ox4b *  node1     Ready   Active        Leader
ehb0...4fvx    node2     Ready   Active
```

???

## Adding nodes using the Docker API

- We don't have to SSH into the other nodes, we can use the Docker API

- Our nodes expose the Docker API over port 2376/tcp,
  <br/>
  protected by TLS mutual authentication

- To connect to other nodes with the Docker API, we will use Docker Machine

  (Our nodes have been suitably pre-configured to be controlled through `node1`)

???

## Docker Machine

- Docker Machine has two primary uses:

  - provisioning cloud instances running the Docker Engine

  - managing local Docker VMs within e.g. VirtualBox

- Docker Machine is purely optional

- It makes it easy to create, upgrade, manage... Docker hosts:

  - on your favorite cloud provider

  - locally (e.g. to test clustering, or different versions)

  - across different cloud providers

???

## Docker Machine basic usage

- We will learn two commands:

  - `docker-machine ls` (list existing hosts)

  - `docker-machine env` (switch to a specific host)

.exercise[

- List configured hosts:
  ```bash
  docker-machine ls
  ```

]

You should see your 5 nodes.

???

## How did we make our 5 nodes show up there?

*For the curious...*

- This was done by our VM provisioning scripts

- After setting up everything else, `node1` adds the 5 nodes
  to the local Docker Machine configuration
  (located in `$HOME/.docker/machine`)

- Nodes are added using [Docker Machine generic driver](https://docs.docker.com/machine/drivers/generic/)

  (It skips machine provisioning and jumps straight to the configuration phase)

- Docker Machine creates TLS certificates and deploys them to the nodes through SSH

???

## Using Docker Machine to communicate with a node

- To select a node, use `eval $(docker-machine env nodeX)`

- This sets a number of environment variables

- To unset these variables, use `eval $(docker-machine env -u)`

.exercise[

- View the variables used by Docker Machine:
  ```bash
  docker-machine env node3
  ```

]

???

## Getting the token

- First, let's store the join token in a variable

.exercise[

- Make sure we talk to the local node:
  ```bash
  eval $(docker-machine env -u)
  ```

- Get the join token:
  ```bash
  TOKEN=$(docker swarm join-token -q worker)
  ```

]

???

## Adding a node through the Docker API

- Now, let's use Docker Machine to switch to `node3` and add it to the cluster

.exercise[

- Communicate with `node3`:
  ```bash
  eval $(docker-machine env node3)
  ```

- Add `node3` to the Swarm:
  ```bash
  docker swarm join --token $TOKEN node1:2377
  ```

]

Note: it can be useful to use a [custom shell prompt](
https://github.com/ebriand/orchestration-workshop/blob/master/prepare-vms/scripts/postprep.rc#L68)
reflecting the `DOCKER_HOST` variable.

???

## Checking that our node is here

- We have to go back to the local node first

.exercise[

- Reset the environment variables:
  ```bash
  eval $(docker-machine env -u)
  ```

- Check that the node is here:
  ```bash
  docker node ls
  ```

]

???

## Under the hood: docker swarm init

When we do `docker swarm init`:

- a keypair is created for the root CA of our Swarm

- a keypair is created for the first node

- a certificate is issued for this node

- the join tokens are created

???

## Under the hood: join tokens

There is one token to *join as a worker*, and another to *join as a manager*.

The join tokens have two parts:

- a secret key (preventing unauthorized nodes from joining)

- a fingerprint of the root CA certificate (preventing MITM attacks)

If a token is compromised, it can be rotated instantly with:
```
docker swarm join-token --rotate <worker|manager>
```

???

## Under the hood: docker swarm join

When a node joins the Swarm:

- it is issued its own keypair, signed by the root CA

- if the node is a manager:

  - it joins the Raft consensus
  - it connects to the current leader
  - it accepts connections from worker nodes

- if the node is a worker:

  - it connects to one of the managers (leader or follower)

???

## Under the hood: cluster communication

- The *control plane* is encrypted with AES-GCM; keys are rotated every 12 hours

- Authentication is done with mutual TLS; certificates are rotated every 90 days

  (`docker swarm update` allows to change this delay or to use an external CA)

- The *data plane* (communication between containers) is not encrypted by default

  (but this can be activated on a by-network basis, using IPSEC,
  leveraging hardware crypto if available)

---

## Under the hood: I want to know more!

Revisit SwarmKit concepts:

- Docker 1.12 Swarm Mode Deep Dive Part 1: Topology
  ([video](https://www.youtube.com/watch?v=dooPhkXT9yI))

- Docker 1.12 Swarm Mode Deep Dive Part 2: Orchestration
  ([video](https://www.youtube.com/watch?v=_F6PSP-qhdA))

Some presentations from the Docker Distributed Systems Summit in Berlin:

- Heart of the SwarmKit: Topology Management
  ([slides](https://speakerdeck.com/aluzzardi/heart-of-the-swarmkit-topology-management))

- Heart of the SwarmKit: Store, Topology & Object Model
  ([slides](http://www.slideshare.net/Docker/heart-of-the-swarmkit-store-topology-object-model))
  ([video](https://www.youtube.com/watch?v=EmePhjGnCXY))

???

## Adding more manager nodes

- Right now, we have only one manager (node1)

- If we lose it, we're SOL

- Let's make our cluster highly available

.exercise[

- Get the manager token and use it to add nodes 4 and 5 to the cluster:
  ```bash
    TOKEN=$(docker swarm join-token -q manager)
    for N in 4 5; do
      eval $(docker-machine env node$N)
      docker swarm join --token $TOKEN node1:2377
    done
  ```

]

---

## You can control the Swarm from any manager node

.exercise[

- Try the following command on a few different nodes:
  ```bash
  ssh nodeX docker node ls
  ```

]

On manager nodes:
<br/>you will see the list of nodes, with a `*` denoting
the node you're talking to.

On non-manager nodes:
<br/>you will get an error message telling you that
the node is not a manager.

As we saw earlier, you can only control the Swarm through a manager node.

---

## Promoting nodes

- Instead of adding a manager node, we can also promote existing workers

- Nodes can be promoted (and demoted) at any time

.exercise[

- See the current list of nodes:
  ```
  docker node ls
  ```

- Promote the two worker nodes to be managers:
  ```
  docker node promote XXX YYY
  ```

]

---

## How many managers do we need?

- 2N+1 nodes can (and will) tolerate N failures
  <br/>(you can have an even number of managers, but there is no point)

--

- 1 manager = no failure

- 3 managers = 1 failure

- 5 managers = 2 failures (or 1 failure during 1 maintenance)

- 7 managers and more = now you might be overdoing it a little bit

---

## Why not have *all* nodes be managers?

- Intuitively, it's harder to reach consensus in larger groups

- With Raft, each write needs to be acknowledged by the majority of nodes

- More nodes = more chance that we will have to wait for some laggard

- Bigger network = more latency

---

## What would McGyver do?

- If some of your machines are more than 10ms away from each other,
  <br/>
  try to break them down in multiple clusters
  (keeping internal latency low)

- Groups of up to 9 nodes: all of them are managers

- Groups of 10 nodes and up: pick 5 "stable" nodes to be managers

- Groups of more than 100 nodes: watch your managers' CPU and RAM

- Groups of more than 1000 nodes:

  - if you can afford to have fast, stable managers, add more of them
  - otherwise, break down your nodes in multiple clusters

---

## What's the upper limit?

- We don't know!

- Internal testing at Docker Inc.: 1000-10000 nodes is fine

  - deployed to a single cloud region

  - one of the main take-aways was *"you're gonna need a bigger manager"*

- Testing by the community: [4700 heterogenous nodes all over the 'net](https://sematext.com/blog/2016/11/14/docker-swarm-lessons-from-swarm3k/)

  - it just works

  - more nodes require more CPU; more containers require more RAM

  - scheduling of large jobs (70000 containers) is slow, though (working on it!)

---

# Running our first Swarm service

- How do we run services? Simplified version:

  `docker run` ‚Üí `docker service create`

.exercise[

- Create a service featuring an Alpine container pinging Google resolvers:
  ```bash
  docker service create alpine ping 8.8.8.8
  ```

- Check where the container was created:
  ```bash
  docker service ps <serviceID>
  ```

]

---

## Checking container logs

- Right now, there is no direct way to check the logs of our container
  <br/>(unless it was scheduled on the current node)

- Look up the `NODE` on which the container is running
  <br/>(in the output of the `docker service ps` command)

.exercise[

- Log into the node *or* use Docker Machine to talk to it:
  ```bash
  ssh nodeX
  ```
  ```bash
  eval $(docker-machine env nodeX)
  ```

]

---

## Viewing the logs of the container

.exercise[

- See that the container is running and check its ID:
  ```bash
  docker ps
  ```

- View its logs:
  ```bash
  docker logs <containerID>
  ```

- Go back to `node1` afterwards by logging out of SSH, or by running:
  ```bash
  eval $(docker-machine env -u)
  ```

]

---

## Scale our service

- Services can be scaled in a pinch with the `docker service update`
  command

.exercise[

- Scale the service to ensure 2 copies per node:
  ```bash
  docker service update <serviceID> --replicas 10
  ```

- Check that we have two containers on the current node:
  ```bash
  docker ps
  ```

]

---

## Expose a service

- Services can be exposed, with two special properties:

  - the public port is available on *every node of the Swarm*,

  - requests coming on the public port are load balanced across all instances.

- This is achieved with option `-p/--publish`; as an approximation:

  `docker run -p ‚Üí docker service create -p`

- If you indicate a single port number, it will be mapped on a port
  starting at 30000
  <br/>(vs. 32768 for single container mapping)

- You can indicate two port numbers to set the public port number
  <br/>(just like with `docker run -p`)

---

## Expose ElasticSearch on its default port

.exercise[

- Create an ElasticSearch service (and give it a name while we're at it):
  ```bash
  docker service create --name search --publish 9200:9200 --replicas 7 \
         elasticsearch`:2`
  ```

- Check what's going on:
  ```bash
  watch docker service ps search
  ```

]

Note: don't forget the **:2**!

The latest version of the ElasticSearch image won't start without mandatory configuration.

---

## Tasks lifecycle

- If you are fast enough, you will be able to see multiple states:

  - assigned (the task has been assigned to a specific node)
  - preparing (right now, this mostly means "pulling the image")
  - running

- When a task is terminated (stopped, killed...) it cannot be restarted

  (A replacement task will be created)

---

![diagram showing what happens during docker service create, courtesy of @aluzzardi](docker-service-create.svg)

---

## Test our service

- We mapped port 9200 on the nodes, to port 9200 in the containers

- Let's try to reach that port!

.exercise[

- Repeat the following command a few times:
  ```bash
  curl localhost:9200
  ```

]

You might get a few `Connection refused` errors; just keep trying.

Each request should be served by a different ElasticSearch instance.
<br/>
(You will see each instance advertising a different name.)

---

## Terminate our services

- Before moving on, we will remove those services

- `docker service rm` can accept multiple services names or IDs

- `docker service ls` can accept the `-q` flag

- A Shell snippet a day keeps the cruft away

.exercise[

- Remove all services with this one liner:
  ```bash
  docker service ls -q | xargs docker service rm
  ```

]

---

class: title

#  Our app on Swarm

---

## What's on the menu?

In this part, we will cover:

- building images for our app,

- shipping those images with a registry,

- running them through the services concept,

- enabling inter-container communication with overlay networks.

---

## Why do we need to ship our images?

- When we do `docker-compose up`, images are built for our services

- Those images are present only on the local node

- We need those images to be distributed on the whole Swarm

- The easiest way to achieve that is to use a Docker registry

- Once our images are on a registry, we can reference them when
  creating our services

---

## Build, ship, and run, for a single service

If we had only one service (built from a `Dockerfile` in the
current directory), our workflow could look like this:

```
docker build -t jpetazzo/doublerainbow:v0.1 .
docker push jpetazzo/doublerainbow:v0.1
docker service create jpetazzo/doublerainbow:v0.1
```

We just have to adapt this to our application, which has 4 services!

---

## The plan

- Build on our local node (`node1`)

- Tag images with a version number

  (timestamp; git hash; semantic...)

- Upload them to a registry

- Create services using the images

---

## Which registry do we want to use?

.small[

- **Docker Hub**

  - hosted by Docker Inc.
  - requires an account (free, no credit card needed)
  - images will be public (unless you pay)
  - located in AWS EC2 us-east-1

- **Docker Trusted Registry**

  - self-hosted commercial product
  - requires a subscription (free 30-day trial available)
  - images can be public or private
  - located wherever you want

- **Docker open source registry**

  - self-hosted barebones repository hosting
  - doesn't require anything
  - doesn't come with anything either
  - located wherever you want

]

???
---

## Using Docker Hub

*If we wanted to use the Docker Hub...*

<!--
```meta
^{
```
-->

- We would set the following environment variable:
  ```bash
  export DOCKER_REGISTRY=jpetazzo
  ```

  (Using *our* Docker Hub login, of course!)

- And we would log into the Docker Hub:
  ```bash
  docker login
  ```

<!--
```meta
^}
```
-->

]

???
???

## Using Docker Trusted Registry

*If we wanted to use DTR, we would...*

- Make sure we have a Docker Hub account

- [Activate a Docker Datacenter subscription](
  https://hub.docker.com/enterprise/trial/)

- Install DTR on our machines

- Set `DOCKER_REGISTRY` to `dtraddress:port/user`

*This is out of the scope of this workshop!*

???

## Using open source registry

- We need to run a `registry:2` container
  <br/>(make sure you specify tag `:2` to run the new version!)

- It will store images and layers to the local filesystem
  <br/>(but you can add a config file to use S3, Swift, etc.)

<!-- -->

- Docker *requires* TLS when communicating with the registry

  - unless for registries on `localhost`

  - or with the Engine flag `--insecure-registry`

<!-- -->

- Our strategy: publish the registry container on port 5000,
  <br/>so that it's available through `localhost:5000` on each node

???

# Deploying a local registry

- We will create a single-instance service, publishing its port
  on the whole cluster

.exercise[

- Create the registry service:
  ```bash
  docker service create --name registry --publish 5000:5000 registry:2
  ```

- Try the following command, until it returns `{"repositories":[]}`:
  ```bash
  curl localhost:5000/v2/_catalog
  ```

]

(Retry a few times, it might take 10-20 seconds for the container to be started. Patience.)

???

## Testing our local registry

- We can retag a small image, and push it to the registry

.exercise[

- Make sure we have the busybox image, and retag it:
  ```bash
  docker pull busybox
  docker tag busybox localhost:5000/busybox
  ```

- Push it:
  ```bash
  docker push localhost:5000/busybox
  ```

]

???

## Checking what's on our local registry

- The registry API has endpoints to query what's there

.exercise[

- Ensure that our busybox image is now in the local registry:
  ```bash
  curl http://localhost:5000/v2/_catalog
  ```

]

The curl command should now output:
```json
{"repositories":["busybox"]}
```

---

## Build, tag, and push our application container images

- Scriptery to the rescue!

.exercise[

- Set `DOCKER_REGISTRY` and `TAG` environment variables to use our local registry

- And run this little for loop:
  ```bash
    DOCKER_REGISTRY=jpetazzo
    TAG=v0.1
    for SERVICE in hasher rng webui worker; do
      docker-compose build $SERVICE
      docker tag dockercoins_$SERVICE $DOCKER_REGISTRY/dockercoins_$SERVICE:$TAG
      docker push $DOCKER_REGISTRY/dockercoins_$SERVICE
    done
  ```

]

---

# Overlay networks

- SwarmKit integrates with overlay networks, without requiring
  an extra key/value store

- Overlay networks are created the same way as before

.exercise[

- Create an overlay network for our application:
  ```bash
  docker network create --driver overlay dockercoins
  ```

- Check existing networks:
  ```bash
  docker network ls
  ```

]

---

## Can you spot the differences?

The networks `dockercoins` and `ingress` are different from the other ones.

Can you see how?

--

- They are using a different kind of ID, reflecting the fact that they
  are SwarmKit objects instead of "classic" Docker Engine objects.

- Their *scope* is `swarm` instead of `local`.

- They are using the overlay driver.

---

## Caveats

.warning[In Docker 1.12, you cannot join an overlay network with `docker run --net ...`.]

Starting with version 1.13, you can, if the network was created with the `--attachable` flag.

*Why is that?*

Placing a container on a network requires allocating an IP address for this container.

The allocation must be done by a manager node (worker nodes cannot update Raft data).

As a result, `docker run --net ...` requires collaboration with manager nodes.

It alters the code path for `docker run`, so it is allowed only under strict circumstances.

---

## Run the application

- First, create the `redis` service; that one is using a Docker Hub image

.exercise[

- Create the `redis` service:
  ```bash
  docker service create --network dockercoins --name redis redis
  ```

]

---

## Run the other services

- Then, start the other services one by one

- We will use the images pushed previously

.exercise[

- Start the other services:
  ```bash
  DOCKER_REGISTRY=localhost:5000
  TAG=v0.1
  for SERVICE in hasher rng webui worker; do
    docker service create --network dockercoins --name $SERVICE \
           $DOCKER_REGISTRY/dockercoins_$SERVICE:$TAG
  done
  ```

]

???
---

## Wait for our application to be up

- We will see later a way to watch progress for all the tasks of the cluster

- But for now, a scrappy Shell loop will do the trick

.exercise[

- Repeatedly display the status of all our services:
  ```bash
  watch "docker service ls -q | xargs -n1 docker service ps"
  ```

- Stop it once everything is running

]

---

## Expose our application web UI

- We need to connect to the `webui` service, but it is not publishing any port

<!--
- Let's re-create the `webui` service, but publish its port 80 this time

.exercise[

- Remove the `webui` service:
  ```bash
  docker service rm webui
  ```

- Re-create it:
  ```bash
    docker service create --network dockercoins --name webui \
           -p 8000:80 $DOCKER_REGISTRY/dockercoins_webui:$TAG
  ```

]
-->

- Let's reconfigure it to publish a port

.exercise[

- Update `webui` so that we can connect to it from outside:
  ```bash
  docker service update webui --publish-add 8000:80
  ```

]

Note: to "de-publish" a port, you would have to specify the container port.
</br>(i.e. in that case, `--publish-rm 80`)

---

## Connect to the web UI

- The web UI is now available on port 8000, *on all the nodes of the cluster*

.exercise[

- Point your browser to any node, on port 8000

]

You might have to wait a bit for the container to be up and running.

Check its status with `docker service ps webui`.

Protip: use `docker service ps webui -a` to see *all* tasks.
<br/>
(Otherwise you only see the ones currently running.)

---

## Scaling the application

- We can change scaling parameters with `docker update` as well

- We will do the equivalent of `docker-compose scale`

.exercise[

- Bring up more workers:
  ```bash
  docker service update worker --replicas 10
  ```

- Check the result in the web UI

]

You should see the performance peaking at 10 hashes/s (like before).

---

## Global scheduling

- We want to utilize as best as we can the entropy generators
  on our nodes

- We want to run exactly one `rng` instance per node

- SwarmKit has a special scheduling mode for that, let's use it

- We cannot enable/disable global scheduling on an existing service

- We have to destroy and re-create the `rng` service

---

## Scaling the `rng` service

.exercise[

- Remove the existing `rng` service:
  ```bash
  docker service rm rng
  ```

- Re-create the `rng` service with *global scheduling*:
  ```bash
    docker service create --name rng --network dockercoins --mode global \
      $DOCKER_REGISTRY/dockercoins_rng:$TAG
  ```

- Look at the result in the web UI

]

Note: if the hash rate goes to zero and doesn't climb back up, try to `rm` and `create` again.

---

## Why do we have to re-create the service to enable global scheduling?

- Enabling it dynamically would make rolling updates semantics very complex

- This might change in the future (after all, it was possible in 1.12 RC!)

- As of Docker Engine 1.13, other parameters requiring to `rm`/`create` the service are:

  - service name

  - hostname

  - network

---

## Checkpoint

- We've seen how to set up a Swarm

- We've used it to host our own registry

- We've built our app container images

- We've used the registry to host those images

- We've deployed and scaled our application

Let's treat ourselves with a nice pat on the back!

--

And carry on, we have much more to see and learn!

---

## Interlude: making our app "Swarm-ready"

This app was written in June 2015. (One year before Swarm mode was released.)

What did we change to make it compatible with Swarm mode?

--

.exercise[

- Go to the app directory:
  ```bash
  cd ~/orchestration-workshop/dockercoins
  ```

- See modifications in the code:
  ```bash
  git log -p --since "4-JUL-2015" -- . ':!*.yml*' ':!*.html'
  ```

]

---

## What did we change in our app since its inception?

- Compose files

- HTML file (it contains an embedded contextual tweet)

- Dockerfiles (to switch to smaller images)

- That's it!

--

*We didn't change a single line of code in this app since it was written.*

--

*The images that were [built in June 2015](
https://hub.docker.com/r/jpetazzo/dockercoins_worker/tags/)
(when the app was written) can still run today ...
<br/>... in Swarm mode (distributed across a cluster, with load balancing) ...
<br/>... without any modification.*

---

## How did we design our app in the first place?

- [Twelve-Factor App](https://12factor.net/) principles

- Service discovery using DNS names

  - Initially implemented as "links"

  - Then "ambassadors"

  - And now "services"

- Existing apps might require more changes!

???

class: title

#  Operating the Swarm

???

## Troubleshooting overlay networks

<!--

## Finding the real cause of the bottleneck

- We want to debug our app as we scale `worker` up and down

-->

- We want to run tools like `ab` or `httping` on the internal network

??

- Ah, if only we had created our overlay network with the `--attachable` flag ...

??

- Oh well, let's use this as an excuse to introduce New Ways To Do Things

---

# Rolling updates

- We want to release a new version of the worker

- We will edit the code ...

- ... build the new image ...

- ... push it to the registry ...

- ... update our service to use the new image

---

## But first...

- Restart the workers

.exercise[

- Just scale back to 10 replicas:
  ```bash
  docker service update worker --replicas 10
  ```

- Check that they're running:
  ```bash
  docker service ps worker
  ```

]

---

## Making changes

.exercise[

- Edit `~/orchestration-workshop/dockercoins/worker/worker.py`

- Locate the line that has a `sleep` instruction

- Reduce the `sleep` from `0.1` to `0.01`

- Save your changes and exit

]

---

## Building and pushing the new image

.exercise[

- Build the new image:
  ```bash
  IMAGE=localhost:5000/dockercoins_worker:v0.01
  docker build -t $IMAGE worker
  ```

- Push it to the registry:
  ```bash
  docker push $IMAGE
  ```

]

Note how the build and push were fast (because caching).

---

## Watching the deployment process

- We will need to open a new window for this

.exercise[

- Look at our service status:
  ```bash
  watch -n1 "docker service ps worker -a | grep -v Shutdown.*Shutdown"
  ```

]

- `docker service ps worker` gives us all tasks
  <br/>(including the one whose current or desired state is `Shutdown`)

- Then we filter out the tasks whose current **and** desired state is `Shutdown`

- Future versions might have fancy filters to make that less tinkerish

---

## Updating to our new image

- Keep the `watch ...` command running!

.exercise[

- In the other window, bring back the workers (if you had stopped them earlier):
  ```bash
  docker service update worker --replicas 10
  ```

- Then, update the service to the new image:
  ```bash
  docker service update worker --image $IMAGE
  ```

]

By default, SwarmKit does a rolling upgrade, one instance at a time.

---

## Changing the upgrade policy

- We can set upgrade parallelism (how many instances to update at the same time)

- And upgrade delay (how long to wait between two batches of instances)

.exercise[

- Change the parallelism to 2 and the delay to 5 seconds:
  ```bash
  docker service update worker --update-parallelism 2 --update-delay 5s
  ```

]

The current upgrade will continue at a faster pace.

---

## Rolling back

- At any time (e.g. before the upgrade is complete), we can rollback

.exercise[

- Rollback to the previous image:
  ```bash
  docker service update worker --image $DOCKER_REGISTRY/dockercoins_worker:v0.1
  ```

- With Docker 1.13, we can also revert to the previous service specification:
  ```bash
  docker service update worker --rollback
  ```

]

Note: if you updated the roll-out parallelism, *rollback* will not rollback to the previous image; it will rollback to the previous roll-out cadence.

---

## Timeline of an upgrade

- SwarmKit will upgrade N instances at a time
  <br/>(following the `update-parallelism` parameter)

- New tasks are created, and their desired state is set to `Ready`
  <br/>.small[(this pulls the image if necessary, ensures resource availability, creates the container ... without starting it)]

- If the new tasks fail to get to `Ready` state, go back to the previous step
  <br/>.small[(SwarmKit will try again and again, until the situation is addressed or desired state is updated)]

- When the new tasks are `Ready`, it sets the old tasks desired state to `Shutdown`

- When the old tasks are `Shutdown`, it starts the new tasks

- Then it waits for the `update-delay`, and continues with the next batch of instances

---

# Dealing with stateful services

- First of all, you need to make sure that the data files are on a *volume*

- Volumes are host directories that are mounted to the container's filesystem

- These host directories can be backed by the ordinary, plain host filesystem ...

- ... Or by distributed/networked filesystems

- In the latter scenario, in case of node failure, the data is safe elsewhere ...

- ... And the container can be restarted on another node without data loss

---

## Building a stateful service experiment

- We will use Redis for this example

- We will expose it on port 10000 to access it easily

.exercise[

- Start the Redis service:
  ```bash
  docker service create --name stateful -p 10000:6379 redis
  ```

- Check that we can connect to it (replace XX.XX.XX.XX with any node's IP address):
  ```bash
  docker run --rm redis redis-cli -h `XX.XX.XX.XX` -p 10000 info server
  ```

]

---

## Accessing our Redis service easily

- Typing that whole command is going to be tedious

.exercise[

- Define a shell alias to make our lives easier:
  ```bash
  alias redis='docker run --rm redis redis-cli -h `XX.XX.XX.XX` -p 10000'
  ```

- Try it:
  ```bash
  redis info server
  ```

]

---

## Basic Redis commands

.exercise[

- Check that the `foo` key doesn't exist:
  ```bash
  redis get foo
  ```

- Set it to `bar`:
  ```bash
  redis set foo bar
  ```

- Check that it exists now:
  ```bash
  redis get foo
  ```

]

---

## Local volumes vs. global volumes

- Global volumes exist in a single namespace

- A global volume can be mounted on any node
  <br/>.small[(bar some restrictions specific to the volume driver in use; e.g. using an EBS-backed volume on a GCE/EC2 mixed cluster)]

- Attaching a global volume to a container allows to start the container anywhere
  <br/>(and retain its data wherever you start it!)

- Global volumes require extra *plugins* (Flocker, Portworx...)

- Docker doesn't come with a default global volume driver at this point

- Therefore, we will fall back on *local volumes*

---

## Local volumes

- We will use the default volume driver, `local`

- As the name implies, the `local` volume driver manages *local* volumes

- Since local volumes are (duh!) *local*, we need to pin our container to a specific host

- We will do that with a *constraint*

.exercise[

- Add a placement constraint to our service:
  ```bash
  docker service update stateful --constraint-add node.hostname==$HOSTNAME
  ```

]

---

## Where is our data?

- If we look for our `foo` key, it's gone!

.exercise[

- Check the `foo` key:
  ```bash
  redis get foo
  ```

- Adding a constraint caused the service to be redeployed:
  ```bash
  docker service ps stateful -a
  ```

]

Note: even if the constraint ends up being a no-op (i.e. not
moving the service), the service gets redeployed.
This ensures consistent behavior.

---

## Setting the key again

- Since our database was wiped out, let's populate it again

.exercise[

- Set `foo` again:
  ```bash
  redis set foo bar
  ```

- Check that it's there:
  ```bash
  redis get foo
  ```

]

---

## Service updates cause containers to be replaced

- Let's try to make a trivial update to the service and see what happens

.exercise[

- Set a memory limit to our Redis service:
  ```bash
  docker service update stateful --limit-memory 100M
  ```

- Try to get the `foo` key one more time:
  ```bash
  redis get foo
  ```

]

The key is blank again!

---

## Service volumes are ephemeral by default

- Let's highlight what's going on with volumes!

.exercise[

- Check the current list of volumes:
  ```bash
  docker volume ls
  ```

- Carry a minor update to our Redis service:
  ```bash
  docker service update stateful --limit-memory 200M
  ```

]

Again: all changes trigger the creation of a new task, and therefore a replacement of the existing container;
even when it is not strictly technically necessary.

---

## The data is gone again

- What happened to our data?

.exercise[

- The list of volumes is slightly different:
  ```bash
  docker volume ls
  ```

]

(You should see one extra volume.)

---

## Assigning a persistent volume to the container

- Let's add an explicit volume mount to our service, referencing a named volume

.exercise[

- Update the service with a volume mount:
  ```bash
    docker service update stateful \
           --mount-add type=volume,source=foobarstore,target=/data
  ```

- Check the new volume list:
  ```bash
  docker volume ls
  ```

]

Note: the `local` volume driver automatically creates volumes.

---

## Checking that persistence actually works across service updates

.exercise[

- Store something in the `foo` key:
  ```bash
  redis set foo barbar
  ```

- Update the service with yet another trivial change:
  ```bash
  docker service update stateful --limit-memory 300M
  ```

- Check that `foo` is still set:
  ```bash
  redis get foo
  ```

]

---

## Recap

- The service must commit its state to disk when being shutdown.red[*]

  (Shutdown = being sent a `TERM` signal)

- The state must be written on files located on a volume

- That volume must be specified to be persistent

- If using a local volume, the service must also be pinned to a specific node

  (And losing that node means losing the data, unless there are other backups)

.footnote[<br/>.red[*]Until recently, the Redis image didn't automatically
persist data. Beware!]

---

## Cleaning up

.exercise[

- Remove the stateful service:
  ```bash
  docker service rm stateful
  ```

- Remove the associated volume:
  ```bash
  docker volume rm foobarstore
  ```

]

Note: we could keep the volume around if we wanted.

---

# Integration with Compose

- We will now see how to streamline service creation

  (i.e. get rid of the `for SERVICE in ...; do docker service create ...` part)

.warning[This is experimental and subject to change!]

---

## Compose file version 3

(New in Docker Engine 1.13)

- Almost identical to version 2

- Can be directly used by a Swarm cluster through `docker stack ...` commands

- Introduces a `deploy` section to pass Swarm-specific parameters

- Resource limits are moved to this `deploy` section

- See [here](https://github.com/aanand/docker.github.io/blob/8524552f99e5b58452fcb1403e1c273385988b71/compose/compose-file.md#upgrading) for the complete list of changes

- Supersedes *Distributed Application Bundles*

  (JSON payload describing an application; could be generated from a Compose file)

---

## Removing everything

- Before deploying using "stacks," let's get a clean slate

.exercise[

- Remove *all* the services:
  ```bash
  docker service ls -q | xargs docker service rm
  ```

]

---

## Our first stack

We need a registry to move images around.

Before, we deployed it with the following command:

```bash
docker service create --publish 5000:5000 registry:2
```

Now, we are going to deploy it with the following stack file:

```yaml
version: "3"

services:
  registry:
    image: registry:2
    ports:
      - "5000:5000"
```

---

## Checking our stack files

- All the stack files that we will use are in the `stacks` directory

.exercise[

- Go to the `stacks` directory:
  ```bash
  cd ~/orchestration-workshop/stacks
  ```

- Check `registry.yml`:
  ```bash
  cat registry.yml
  ```

]

---

## Deploying our first stack

- All stack manipulation commands start with `docker stack`

- Under the hood, they map to `docker service` commands

- Stacks have a *name* (which also serves as a namespace)

- Stacks are specified with the aforementioned Compose file format version 3

.exercise[

- Deploy our local registry:
  ```bash
  docker stack deploy registry --compose-file registry.yml
  ```

]

---

## Inspecting stacks

- `docker stack ps` shows the detailed state of all services of a stack

.exercise[

- Check that our registry is running correctly:
  ```bash
  docker stack ps registry
  ```

- Confirm that we get the same output with the following command:
  ```bash
  docker service ps registry_registry
  ```

]

---

## Specifics of stack deployment

Our registry is not *exactly* identical to the one deployed with `docker service create`!

- Each stack gets its own overlay network

- Services of the task are connected to this network
  <br/>(unless specified differently in the Compose file)

- Services get network aliases matching their name in the Compose file
  <br/>(just like when Compose brings up an app specified in a v2 file)

- Services are explicitly named `<stack_name>_<service_name>`

- Services and tasks also get an internal label indicating which stack they belong to

---

## Building and pushing stack services

- We are going to use the `build` + `image` trick that we showed earlier:

  ```bash
  docker-compose -f my_stack_file.yml build
  docker-compose -f my_stack_file.yml push
  docker stack deploy my_stack --compose-file my_stack_file.yml
  ```

.exercise[

- Try it:
  ```bash
  docker-compose -f dockercoins.yml build
  ```

]

--

It doesn't work!?!

---

## Upgrading Compose

- Compose file format version 3 is not supported in Compose 1.9

- We have to use 1.10 (which is not released yet)

.exercise[

- Upgrade Compose, using the `master` branch:
  ```bash
  pip install git+git://github.com/docker/compose
  ```

- Re-hash our `$PATH`:
  ```bash
  hash docker-compose
  ```

]

---

## Trying again

- We can now build and push our container images

.exercise[

- Build our application:
  ```bash
  docker-compose -f dockercoins.yml build
  ```

- Push the images to our registry:
  ```bash
  docker-compose -f dockercoins.yml push
  ```

]

Let's have a look at the `dockercoins.yml` file while this is building and pushing.

---

```yaml
version: "3"

services:
  rng:
    build: dockercoins/rng
    image: ${REGISTRY_SLASH-localhost:5000/}rng${COLON_TAG-:latest}
    logging:
      driver: gelf
      options:
        gelf-address: udp://localhost:12201
    deploy:
      mode: global
  ...
  redis:
    image: redis
  ...
  worker:
    build: dockercoins/worker
    image: ${REGISTRY_SLASH-localhost:5000/}worker${COLON_TAG-:latest}
    ...
    deploy:
      replicas: 10
```

---

## Deploying the application

- Now that the images are on the registry, we can deploy our application stack

.exercise[

- Create the application stack:
  ```bash
  docker stack deploy dockercoins --compose-file dockercoins.yml
  ```

]

We can now connect to any of our nodes on port 8000, and we will see the familiar hashing speed graph.

---

## Caveats

- Compose file format v3 is very new, and not written in the stone yet

- It exists because nobody wants to maintain redundant files if they can avoid it

- Some features are not fully supported yet:

  - logging, see [#29116](https://github.com/docker/docker/issues/29116)

  - secrets

- You can re-run `docker stack deploy` to update a stack

- ... But unsupported features will be wiped each time you redeploy (!)

  (This will likely be fixed/improved soon)

---

## Maintaining multiple environments

There are many ways to handle variations between environments.

- Compose loads `docker-compose.yml` and (if it exists) `docker-compose.override.yml`

- Compose can load alternate file(s) by setting the `-f` flag or the `COMPOSE_FILE` environment variable

- Compose files can *extend* other Compose files, selectively including services:

  ```yaml
    web:
      extends:
        file: common-services.yml
        service: webapp
  ```

See [this documentation page](https://docs.docker.com/compose/extends/) for more details about these techniques.

---

## Disk space management: `docker system df`

- Shows disk usage for images, containers, and volumes

- Breaks down between *active* and *reclaimable* categories

.exercise[

- Check how much disk space is used at the end of the workshop:
  ```bash
  docker system df
  ```

]

Note: `docker system` is new in Docker Engine 1.13.

---

## Reclaiming unused resources: `docker system prune`

- Removes stopped containers

- Removes dangling images (that don't have a tag associated anymore)

- Removes orphaned volumes

- Removes empty networks

.exercise[

- Try it:
  ```bash
  docker system prune -f
  ```

]

Note: `docker system prune -a` will also remove *unused* images.

---

class: title

# Thanks! <br/> Questions?

<!--
## [@jpetazzo](https://twitter.com/jpetazzo) <br/> [@docker](https://twitter.com/docker)
-->

<!--
## AJ ([@s0ulshake](https://twitter.com/s0ulshake)) <br/> J√©r√¥me ([@jpetazzo](https://twitter.com/jpetazzo)) <br/> Tiffany ([@tiffanyfayj](https://twitter.com/tiffanyfayj))
-->

    </textarea>
    <script src="remark-0.14.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true
      });
    </script>
  </body>
</html>
